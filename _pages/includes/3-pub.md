# üìù Selected Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/multimodal_iclr2025.png' alt="sym" width="80%" ></div></div>
<div class='paper-box-text' markdown="1">


[Causal Representation Learning from Multimodal Biomedical Observations](https://openreview.net/pdf?id=hjROBHstZ3)

**Yuewen Sun<sup>\*</sup>**, Lingjing Kong<sup>\*</sup>, Guangyi Chen, Loka Li, Gongxu Luo, Zijian Li, Yixuan Zhang, Yujia Zheng, Mengyue Yang, Petar Stojanov, Eran Segal, Eric P. Xing, Kun Zhang

(<sup>\*</sup>Equal contribution)

[**Project**](https://openreview.net/forum?id=hjROBHstZ3&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We aim to identify the latent state-transition processes from observed state-action trajectories, facilitating the learning of personalized RL policies.
- Theoretical identifiability is guaranteed under both finite and infinite latent factor conditions, supporting the framework‚Äôs robustness.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/imdp_neurips24.png' alt="sym" width="80%" ></div></div>
<div class='paper-box-text' markdown="1">


[Identifying Latent State-Transition Processes for Individualized RL](https://openreview.net/pdf?id=kREpCQtHdN)

**Yuewen Sun**, Biwei Huang, Yu Yao, Donghuo Zeng, Xinshuai Dong, Songyao Jin, Boyang Sun, Roberto Legaspi, Kazushi Ikeda, Peter Spirtes, Kun Zhang

[**Project**](https://openreview.net/forum?id=kREpCQtHdN&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We aim to identify the latent state-transition processes from observed state-action trajectories, facilitating the learning of personalized RL policies.
- Theoretical identifiability is guaranteed under both finite and infinite latent factor conditions, supporting the framework‚Äôs robustness.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='images/acamda_aaai24.png' alt="sym" width="80%" ></div></div>
<div class='paper-box-text' markdown="1">

[ACAMDA: Improving Data Efficiency in Reinforcement Learning Through Guided Counterfactual Data Augmentation](https://ojs.aaai.org/index.php/AAAI/article/view/29442/30719)

**Yuewen Sun**, Erli Wang, Biwei Huang, Chaochao Lu, Lu Feng, Changyin Sun, Kun Zhang 

[**Project**](https://ojs.aaai.org/index.php/AAAI/article/view/29442) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We employ counterfactual reasoning to generate augmented datasets, enabling agents to make unbiased decisions, and model causal relationships within the system to ensure adaptability across heterogeneous environments.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2022</div><img src='images/leap_iclr22.png' alt="sym" width="80%" ></div></div>
<div class='paper-box-text' markdown="1">

[Learning Temporally Causal Latent Processes from General Temporal Data](https://arxiv.org/abs/2110.05428)

Weiran Yao<sup>\*</sup>, **Yuewen Sun<sup>\*</sup>**, Alex Ho, Changyin Sun, Kun Zhang 

(<sup>\*</sup>Equal contribution)

[**Project**](https://github.com/weirayao/leap) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose two provable conditions under which temporally causal latent processes can be identified from their observed nonlinear mixtures.
- We develop a theoretically-grounded training framework that enforces the assumed
conditions through proper constraints.

</div>
</div>

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->